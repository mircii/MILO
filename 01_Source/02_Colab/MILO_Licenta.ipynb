{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mircii/MILO/blob/main/01_src/02_colab/MILO_Licenta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKKw4DwK1TlC"
      },
      "source": [
        "#**MILO**\n",
        "**Release v1.0**\n",
        "\n",
        "- Machine Learning Model\n",
        "- Conversational NLP\n",
        "- Trained on https://www.kaggle.com/datasets/kreeshrajani/3k-conversations-dataset-for-chatbot translated in Romanian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykcOYNL72gBT"
      },
      "source": [
        "## Translating to romanian and converting the dataset to .json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6pREixhVyo7r",
        "outputId": "8df3eb41-314b-4277-efff-4ac8a782a8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=a74c262fc821d9b686f71b0441f6236d3a5e4ded850c9650905e82323e6d6214\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.13 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.4.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTxfcqUUadHP",
        "outputId": "52aa2391-898b-4246-df58-f9cf8cbab7a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "Error during translation: The read operation timed out\n",
            "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
            "Translated conversations have been saved to 'Conversation_romanian.json'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "import json\n",
        "import time  # To add a delay between requests\n",
        "import random  # To introduce variability in delays\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Conversation.csv')\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to translate text\n",
        "def translate_text(text, target_language=\"ro\"):\n",
        "    if not text:  # Check if the text is empty or None\n",
        "        return \"\"  # Return empty string if there's no text to translate\n",
        "\n",
        "    try:\n",
        "        translated = translator.translate(text, src='en', dest=target_language)\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during translation: {e}\")\n",
        "        return \"\"  # Return empty string in case of error\n",
        "\n",
        "# Create a list to store the translated conversations\n",
        "translated_conversations = []\n",
        "\n",
        "# Loop through each row in the dataframe\n",
        "for index, row in df.iterrows():\n",
        "    # Extracting question and answer\n",
        "    question = row.get('question', '')\n",
        "    answer = row.get('answer', '')\n",
        "\n",
        "    # Translate the question and answer\n",
        "    question_ro = translate_text(question, 'ro')\n",
        "    answer_ro = translate_text(answer, 'ro')\n",
        "\n",
        "    # Add the translated conversation to the list\n",
        "    translated_conversations.append({\n",
        "        \"question_ro\": question_ro,\n",
        "        \"answer_ro\": answer_ro\n",
        "    })\n",
        "\n",
        "\n",
        "# Save the translated conversations to a JSON file\n",
        "with open('Conversation_romanian.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(translated_conversations, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Translated conversations have been saved to 'Conversation_romanian.json'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZAhsQ7X463j"
      },
      "source": [
        "## Preprocessing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VHd2dBvr5Ay_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extracting the questions and answers\n",
        "with open('Conversation_romanian.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "questions = [item['question_ro'] for item in data]\n",
        "answers = [item['answer_ro'] for item in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNsPkn3D9bxN",
        "outputId": "ea64ca60-24c4-4120-b919-18c88ee071a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 questions: ['Bună, ce mai faci?', 'Sunt bine.Ce zici de tine?', 'Sunt destul de bun.Mulțumesc că ai întrebat.', 'nici o problemă.Deci, cum ai fost?', 'Am fost grozav.şi tu?', 'Am fost bun.Sunt la școală chiar acum.', 'La ce școală mergi?', 'Merg la PCC.', 'Îți place acolo?', 'e în regulă.Este un campus cu adevărat mare.']\n",
            "First 10 answers: ['Sunt bine.Ce zici de tine?', 'Sunt destul de bun.Mulțumesc că ai întrebat.', 'nici o problemă.Deci, cum ai fost?', 'Am fost grozav.şi tu?', 'Am fost bun.Sunt la școală chiar acum.', 'La ce școală mergi?', 'Merg la PCC.', 'Îți place acolo?', 'e în regulă.Este un campus cu adevărat mare.', 'Noroc cu școala.']\n"
          ]
        }
      ],
      "source": [
        "# Visualizing the first 10 questions and answers\n",
        "\n",
        "print(\"First 10 questions:\", questions[:10])\n",
        "print(\"First 10 answers:\", answers[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsscxNl6PSo"
      },
      "source": [
        "## Tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HSa4fK8o5z21"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "\n",
        "# Fit the tokenizer on both questions and answers\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1  # +1 to include padding/OOV token\n",
        "\n",
        "# Convert text to sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDoAeRDl8kd4",
        "outputId": "31f7ec42-93a9-4a55-a861-43e32cc4adc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 3598\n",
            "First 10 words in vocabulary: [('<OOV>', 1), ('de', 2), ('nu', 3), ('să', 4), ('o', 5), ('este', 6), ('a', 7), ('ce', 8), ('că', 9), ('am', 10)]\n"
          ]
        }
      ],
      "source": [
        "# Visualizing the vocabulary\n",
        "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(\"First 10 words in vocabulary:\", list(word_index.items())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cypY4LsX8uml"
      },
      "source": [
        "## Padding the sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hH3lvXyP8yWo"
      },
      "outputs": [],
      "source": [
        "MAX_sequence_length = max(len(seq) for seq in question_sequences + answer_sequences)\n",
        "\n",
        "question_padded = pad_sequences(question_sequences, maxlen=MAX_sequence_length, padding='post')\n",
        "answer_padded = pad_sequences(answer_sequences, maxlen=MAX_sequence_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ9PZdaz9KIl",
        "outputId": "3c9d106d-d5ca-4337-a91a-1850c1162129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First padded question sequence: [95  8 21 82  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "First padded answer sequence: [ 19  20   8 229   2  67   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n"
          ]
        }
      ],
      "source": [
        "# Visualizing the padding\n",
        "\n",
        "print(\"First padded question sequence:\", question_padded[0])\n",
        "print(\"First padded answer sequence:\", answer_padded[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcJq76t--G0_"
      },
      "source": [
        "## Training & Testing Sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zkyS-4rR-Wly"
      },
      "outputs": [],
      "source": [
        "# Using train test spilt in oreder to create the sets\n",
        "q_train, q_test, a_train, a_test = train_test_split(question_padded, answer_padded, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm95Ijq3-2-N",
        "outputId": "7599e465-c1ab-48c7-a31d-8eea462f9b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training questions shape: (2980, 21)\n",
            "Training answers shape: (2980, 21)\n",
            "Testing questions shape: (745, 21)\n",
            "Testing answers shape: (745, 21)\n",
            "Length of training questions: 2980\n",
            "Length of training answers: 2980\n",
            "Length of testing questions: 745\n",
            "Length of testing answers: 745\n"
          ]
        }
      ],
      "source": [
        "# Visualizing the train & test sets shapres and lengths\n",
        "print(\"Training questions shape:\", q_train.shape)\n",
        "print(\"Training answers shape:\", a_train.shape)\n",
        "print(\"Testing questions shape:\", q_test.shape)\n",
        "print(\"Testing answers shape:\", a_test.shape)\n",
        "\n",
        "print(\"Length of training questions:\", len(q_train))\n",
        "print(\"Length of training answers:\", len(a_train))\n",
        "print(\"Length of testing questions:\", len(q_test))\n",
        "print(\"Length of testing answers:\", len(a_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcCsF2Kk_sBK"
      },
      "source": [
        "## Building the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeeKyqSY_K9k",
        "outputId": "b29e8cd5-2672-457f-b04e-9da1b5beb287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 554ms/step - accuracy: 0.6289 - loss: 3.9717 - val_accuracy: 0.6877 - val_loss: 2.3644\n",
            "Epoch 2/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 480ms/step - accuracy: 0.6913 - loss: 2.2690 - val_accuracy: 0.6913 - val_loss: 2.3545\n",
            "Epoch 3/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 452ms/step - accuracy: 0.6943 - loss: 2.2044 - val_accuracy: 0.6917 - val_loss: 2.4024\n",
            "Epoch 4/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 443ms/step - accuracy: 0.6954 - loss: 2.1312 - val_accuracy: 0.6921 - val_loss: 2.3905\n",
            "Epoch 5/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 499ms/step - accuracy: 0.6949 - loss: 2.1091 - val_accuracy: 0.6904 - val_loss: 2.4131\n",
            "Epoch 6/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 452ms/step - accuracy: 0.6944 - loss: 2.0890 - val_accuracy: 0.6906 - val_loss: 2.4540\n",
            "Epoch 7/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 465ms/step - accuracy: 0.6985 - loss: 2.0379 - val_accuracy: 0.6883 - val_loss: 2.4727\n",
            "Epoch 8/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 431ms/step - accuracy: 0.6984 - loss: 2.0222 - val_accuracy: 0.6920 - val_loss: 2.5244\n",
            "Epoch 9/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 479ms/step - accuracy: 0.6941 - loss: 2.0224 - val_accuracy: 0.6897 - val_loss: 2.5603\n",
            "Epoch 10/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 464ms/step - accuracy: 0.6984 - loss: 1.9805 - val_accuracy: 0.6901 - val_loss: 2.5972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6c8a704690>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Defining some Hyperparameters\n",
        "EMBEDDING_DIM = 256\n",
        "GRU_UNITS = 128\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "MILO_01 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
        "    tf.keras.layers.GRU(GRU_UNITS, return_sequences=True),\n",
        "    tf.keras.layers.GRU(GRU_UNITS, return_sequences=True),\n",
        "    tf.keras.layers.Dense(VOCAB_SIZE)\n",
        "])\n",
        "\n",
        "MILO_01.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.05, patience=1, restore_best_weights=True)\n",
        "\n",
        "MILO_01.fit(q_train, a_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(q_test, a_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "9QAEFfkwBaOp",
        "outputId": "0b741f08-d7d1-492f-8701-94d8eb858ff9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m921,088\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m148,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m99,072\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m3598\u001b[0m)            │         \u001b[38;5;34m464,142\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">921,088</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">148,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3598</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">464,142</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,897,580\u001b[0m (18.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,897,580</span> (18.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,632,526\u001b[0m (6.23 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,632,526</span> (6.23 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,265,054\u001b[0m (12.46 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,265,054</span> (12.46 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "MILO_01.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "NP35-xY6exMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MILO_01.save('MILO_01.keras')"
      ],
      "metadata": {
        "id": "YSxQMMW8e2GX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-5jKiGGKqXf"
      },
      "source": [
        "## Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "_F0EecoQKtgY",
        "outputId": "daecf442-ef61-41d6-9d37-a515259c626b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3536ec4e7973>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "def chatbot_response(user_input):\n",
        "  input_sequence = tokenizer.texts_to_sequences([user_input])\n",
        "  padded_sequence = pad_sequences(input_sequence, maxlen=MAX_sequence_length, padding='post')\n",
        "  predicted_sequence = MILO_01.predict(padded_sequence)\n",
        "  response_text = decode_response(predicted_sequence)\n",
        "  return response_text\n",
        "\n",
        "def decode_response(predicted_sequence):\n",
        "  predicted_indices = np.argmax(predicted_sequence, axis=-1)\n",
        "  response_words = [tokenizer.index_word.get(idx, '') for idx in predicted_indices[0]]\n",
        "  return ' '.join(response_words)\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    response = chatbot_response(user_input)\n",
        "    print(\"MILO_01:\", response)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1HlH4jtt_Qz5CNFVyLx5s0heov3Bu5uA7",
      "authorship_tag": "ABX9TyPnA9Frp1R4uyjkhB2qvJsk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}